# DATA 310 Applied Machine Learning - Project 1 - Good or Bad Housing Deals

#### In this project, we will scrape 400 houses from a city of our choice via Zillow, as we extract some usable predictors including the number of beds, number of bathrooms, and total square footage. We will train a new model with our 400 observations, with the goal of predicting the reasonable housing prices of each house and assess if it is a good deal or not with the price difference. At the end of this project, with the help of some plots, we will analyze the model and evaluate each predictor. Moreover, we will rank all homes from best to worst deals.


#### To start off, letâ€™s talk about the housing data that are collected from Zillow. I chose Phoenix, Arizona as the city to collect information on houses. We first created the data frame with the variables as we add the content to the corresponding variables. We cleaned the data by removing the Html tags that wrap around the information that we want. With all the preprocessing, we generate a CSV file that contains the data for the response variable - price(in thousand) and three independent variables/ predictors.  Figure 1 displays the descriptive statistics of the 400 houses in Phoenix. We can see that the average price of housing in Phoenix is $465,706, the median is $356,000. This difference reflects the existence of luxurious and more expensive houses in the city, which raises the average price of the housing. We will take a closer look at these expensive houses and see if they worth the price. Interestingly, the data also shows that most of the houses from the sample tend to have 4 bedrooms. 

#### To analyze the data, we load the dataset that we exported earlier and utilize the TensorFlow module to train the model. With the Keras function, we are able to compile the three layers, which are the three predictors into the training model. After we train the model, we predict the housing prices and insert the predicted price as a new column in the data frame, as well as the price difference, which is calculated as predicted price subtract by the original price. To get the good deals and bad deals, I subset the data frame into two data frames, one with the price difference greater than 0, which are the good deals. There are 298 houses belongs to this subset. The other 102 houses have a price difference smaller than 0, which are generally bad deals or at least overprice.


#### Assuming that price difference is the only indicator to determine a good deal or not, we can generalize some characteristics of the predictors based on the top 100 good deals in Phoenix. The house typically has four bedrooms, three bathrooms, and approximately 2,950 square footage, which has a predicted price of around $600,000. If someone is looking for a house that has similar dimensions, anything less than $600,000 should be prioritized or at least worth a second thought. In conclusion, this model provides a broad prediction on housing prices for the datasets we collected. There are different approaches to improving the model. It can be more practical and accurate if we scrape more data and perhaps change up the sequence of the layer to determine the strength of each predictor. In fact, there might be some typos or errors from the website that needs to be addressed. In addition, we should also examine more predictors to avoid any systematic bias. 

  
## Appendix 
![Figure 1: Descriptive Statistics table](https://github.com/cning0506/DATA-310_Applied_Machine_Learning/blob/main/Summary%20stat%20table.PNG)
